{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import requests\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1- Write a python program which searches all the product under a particular product from www.amazon.in. The product name to be searched will be taken as input from user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"C://chromedriver.exe\") # Loading chrome driver\n",
    "driver.get(\"https://www.amazon.in/\") # website from which we will scrape data\n",
    "    \n",
    "# Finding search input and giving input\n",
    "search = driver.find_element_by_id(\"twotabsearchtextbox\").send_keys(str(input(\"Please enter Product to search:  \")))\n",
    "time.sleep(5)\n",
    "    \n",
    "# clicking on search button\n",
    "search_btn = driver.find_element_by_xpath(\"//div[@class='nav-search-submit nav-sprite']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) In the above question, now scrape the following details of each product listed in first 3 pages of your search results and save it in a dataframe and csv. In case if any product vertical has less than 3 pages in search results then scrape all the products available under that product vertical. Details to be scraped are: \"Brand Name\", \"Name of the Product\", \"Rating\", \"No. of \n",
    "Ratings\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\", \"Other Details\"and “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Open list\n",
    "brand = []\n",
    "prod_name = []\n",
    "ratings = []\n",
    "no_of_ratings = []\n",
    "price = []\n",
    "ret_exc = []\n",
    "exp_del = []\n",
    "avail = []\n",
    "oth_det = []\n",
    "prod_url = []\n",
    "\n",
    "def Amazon(): # Defining Amazon Function\n",
    "    for x in range(0,3): # Creating loop to extract each product link from first 3 pages. \n",
    "        for j in driver.find_elements_by_xpath(\"//h2[@class = 'a-size-mini a-spacing-none a-color-base s-line-clamp-2']/a\"):\n",
    "            prod_url.append(j.get_attribute('href'))\n",
    "        \n",
    "        # Moving to the next page\n",
    "        next_url = driver.find_element_by_xpath(\"//li[@class = 'a-last']/a\").get_attribute('href') \n",
    "        driver.get(next_url)\n",
    "        \n",
    "        \n",
    "\n",
    "# Calling Function\n",
    "Amazon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking length of product url\n",
    "len(prod_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in prod_url: # Loop for extracting data from each link\n",
    "    driver.get(i)\n",
    "\n",
    "                               \n",
    "    try: # Extracting Product Name\n",
    "        prodname = driver.find_element_by_xpath(\"//h1[@class = 'a-size-large a-spacing-none']/span\")\n",
    "        prod_name.append(prodname.text)\n",
    "    except NoSuchElementException as e:\n",
    "        prod_name.append(\"-\") # If no element, it will be replaced by '-'\n",
    "            \n",
    "       \n",
    "    try:# Extracting Brand Name\n",
    "        bran = driver.find_element_by_xpath(\"//a[@id = 'bylineInfo' and @class = 'a-link-normal']\")\n",
    "        brand.append(bran.text)\n",
    "    except NoSuchElementException as e:\n",
    "        brand.append(\"-\")    # If no element, it will be replaced by '-'\n",
    "            \n",
    "    try:# Extracting Ratings\n",
    "        rate = driver.find_element_by_xpath(\"//span[@id ='acrCustomerReviewText']\")\n",
    "        rate.click()\n",
    "        Ratings = driver.find_element_by_xpath(\"//span[@class='a-size-medium a-color-base']\")\n",
    "        ratings.append(Ratings.text)\n",
    "    except NoSuchElementException as e:\n",
    "        ratings.append(\"-\") # If no element, it will be replaced by '-'\n",
    "            \n",
    "        \n",
    "    try:# Extracting No of Ratings\n",
    "        no_of_rat = driver.find_element_by_xpath(\"//span[@id ='acrCustomerReviewText']\")\n",
    "        no_of_ratings.append(no_of_rat.text)\n",
    "    except NoSuchElementException as e:\n",
    "        no_of_ratings.append(\"-\") # If no element, it will be replaced by '-'\n",
    "                \n",
    "    try: # Extracting Price\n",
    "        pri = driver.find_element_by_xpath(\"//span[@id = 'priceblock_ourprice' and @class = 'a-size-medium a-color-price priceBlockBuyingPriceString']\")\n",
    "        price.append(pri.text)\n",
    "    except NoSuchElementException as e:\n",
    "        price.append(\"-\") # If no element, it will be replaced by '-'\n",
    "                \n",
    "    try:# Extracting return/exchange data\n",
    "        exc = driver.find_element_by_xpath(\"//a[@class = 'a-size-small a-link-normal a-text-normal']\")\n",
    "        ret_exc.append(exc.text)\n",
    "    except NoSuchElementException as e:\n",
    "        ret_exc.append(\"-\") # If no element, it will be replaced by '-'\n",
    "            \n",
    "            \n",
    "    try:# Extracting expected delivery of product\n",
    "        ex_del = driver.find_element_by_xpath(\"//div[@id = 'ddmDeliveryMessage' and @class = 'a-section a-spacing-mini']/b\")\n",
    "        exp_del.append(ex_del.text)\n",
    "    except NoSuchElementException as e:\n",
    "        exp_del.append(\"-\") # If no element, it will be replaced by '-'\n",
    "            \n",
    "            \n",
    "    try: # Extracting availability of product\n",
    "        aval = driver.find_element_by_xpath(\"//div[@id = 'availability' and @class = 'a-section a-spacing-base']/span\")\n",
    "        avail.append(aval.text)\n",
    "    except NoSuchElementException as e:\n",
    "        avail.append(\"-\") # If no element, it will be replaced by '-'\n",
    "                \n",
    "            \n",
    "    try: # Extracting Other details of product\n",
    "        other_det = driver.find_element_by_xpath(\"//ul[@class = 'a-unordered-list a-vertical a-spacing-mini']\")\n",
    "        oth_det.append(other_det.text)\n",
    "    except NoSuchElementException as e:\n",
    "        oth_det.append(\"-\") # If no element, it will be replaced by '-'\n",
    "                \n",
    "    continue # command to return i till i is max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = pd.DataFrame({}) # saving scraped data into dataframe.\n",
    "search['Brand'] = brand\n",
    "search['Product Name'] = prod_name\n",
    "search['Ratings'] = ratings\n",
    "search['No. of Ratings'] = no_of_ratings\n",
    "search['Price'] = price\n",
    "search['Return/Exchange'] = ret_exc\n",
    "search['Expected Delivery'] = exp_del\n",
    "search['Availibility'] = avail\n",
    "search['Other Details'] = oth_det\n",
    "search['Product URL'] = prod_url\n",
    "print(search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3) Write a python program to access the search bar and search button on images.google.com and scrape 100 images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Images Downloaded Successfully\n",
      "100 Images Downloaded Successfully\n",
      "100 Images Downloaded Successfully\n"
     ]
    }
   ],
   "source": [
    "img_urls = [] # open list to save image urls\n",
    "keywords = ['fruits','cars','Machine Learning'] # Keywords to be searched\n",
    "\n",
    "driver = webdriver.Chrome(\"C://chromedriver.exe\") # Loading chrome driver\n",
    "driver.get(\"https://images.google.com/\") # website from which we will scrape data\n",
    "\n",
    "initial_url = driver.current_url # command to return for changing keywords   \n",
    "\n",
    "for i in keywords:\n",
    "        driver.find_element_by_xpath(\"//input[@class = 'gLFyf gsfi']\").send_keys(i) # Targeting search bar and sending keywords\n",
    "            \n",
    "        clicks = driver.find_element_by_xpath(\"//span[@class = 'z1asCe MZy1Rb']\")\n",
    "        clicks.click() # Clicking on search icon\n",
    "    \n",
    "    # Scrolling to the end of page \n",
    "        last_height = driver.execute_script('return document.body.scrollHeight')\n",
    "        while True:\n",
    "            driver.execute_script('window.scrollTo(0,document.body.scrollHeight)')\n",
    "            time.sleep(2)\n",
    "            new_heignt = driver.execute_script('return document.body.scrollHeight')\n",
    "               \n",
    "            if new_heignt == last_height:\n",
    "                break\n",
    "            last_height = new_heignt\n",
    "        \n",
    "        images = driver.find_elements_by_xpath('//img[@class=\"rg_i Q4LuWd\"]')\n",
    "    # As the thumbnails of images are in src, so first we will extract src\n",
    "        for image in images:\n",
    "            source= image.get_attribute('src')\n",
    "            if source is not None:\n",
    "                if(source[0:99] == 'http'):\n",
    "                    img_urls.append(source) # saving searched src in open list\n",
    "    \n",
    "    \n",
    "        path = \"C:/Users/vikas/images/vikas/\" # Path where we will save the images\n",
    "        for j in range(len(img_urls)): # loop for clicking on each images\n",
    "            if j >= 100: # if the length of img_urls = 100, loop will break\n",
    "                break\n",
    "            response= requests.get(img_urls[j])\n",
    "            file = open(path + i +str(j)+\".jpg\", \"wb\") # Saving images\n",
    "            file.write(response.content)\n",
    "        print(\"100 Images of\", i ,\"Downloaded Successfully\")\n",
    "        \n",
    "        driver.get(initial_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4) Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape  following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Display Resolution”, “Processor”, “Processor Cores”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “-\". Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C://chromedriver.exe\") # loading webdriver\n",
    "driver.get(\"https://www.flipkart.com/\") # Website from which we have to scrape data\n",
    "   \n",
    "# Closing login pop-up\n",
    "pop_btn = driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _2doB4z']\").click()\n",
    "   \n",
    "# finding search job tab and giving input sunglasses\n",
    "search_prod = driver.find_element_by_xpath(\"//input[@type = 'text']\").send_keys(\"oppo f19 pro\") \n",
    "    \n",
    "# Clicking on Search button\n",
    "search_btn = driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\").click()\n",
    "        \n",
    "time.sleep(3)\n",
    "    \n",
    "# extracting href of products\n",
    "url = driver.find_elements_by_xpath(\"//a[@class = '_1fQZEK']\")\n",
    "\n",
    "url_list = []\n",
    "for i in url:\n",
    "    link = i.get_attribute('href')\n",
    "    url_list.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating open lists\n",
    "brand = []\n",
    "smartphone = []\n",
    "colour = []\n",
    "RAM = []\n",
    "storage = []\n",
    "Pri_Cam = []\n",
    "Sec_Cam = []\n",
    "Display_Size = []\n",
    "Display_Resolution = []\n",
    "Processor = []\n",
    "Processor_Cores = []\n",
    "Battery = []\n",
    "price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Brand  \\\n",
      "0                 OPPO F19 Pro   \n",
      "1                 OPPO F19 Pro   \n",
      "2                 OPPO F19 Pro   \n",
      "3                 OPPO F19 Pro   \n",
      "4  AI Highlight Portrait Video   \n",
      "5  AI Highlight Portrait Video   \n",
      "6                            -   \n",
      "7                            -   \n",
      "\n",
      "                                         Description           Color   RAM  \\\n",
      "0  OPPO F19 Pro (Crystal Silver, 256 GB)  (8 GB RAM)  Crystal Silver  8 GB   \n",
      "1     OPPO F19 Pro (Fluid Black, 128 GB)  (8 GB RAM)     Fluid Black  8 GB   \n",
      "2     OPPO F19 Pro (Fluid Black, 256 GB)  (8 GB RAM)     Fluid Black  8 GB   \n",
      "3  OPPO F19 Pro (Crystal Silver, 128 GB)  (8 GB RAM)  Crystal Silver  8 GB   \n",
      "4  OPPO F19 Pro+ 5G (Fluid Black, 128 GB)  (8 GB ...     Fluid Black  8 GB   \n",
      "5  OPPO F19 Pro+ 5G (Space Silver, 128 GB)  (8 GB...    Space Silver  8 GB   \n",
      "6     OPPO F19PRO+ (Fluid Black, 128 GB)  (8 GB RAM)     Fluid Black  8 GB   \n",
      "7    OPPO F19PRO+ (Space Silver, 128 GB)  (8 GB RAM)    Space Silver  8 GB   \n",
      "\n",
      "  Storage          Primary Camara   Secondary Camera          Display Size  \\\n",
      "0  256 GB  48MP + 8MP + 2MP + 2MP  16MP Front Camera  16.33 cm (6.43 inch)   \n",
      "1  128 GB  48MP + 8MP + 2MP + 2MP  16MP Front Camera  16.33 cm (6.43 inch)   \n",
      "2  256 GB  48MP + 8MP + 2MP + 2MP  16MP Front Camera  16.33 cm (6.43 inch)   \n",
      "3  128 GB  48MP + 8MP + 2MP + 2MP  16MP Front Camera  16.33 cm (6.43 inch)   \n",
      "4  128 GB  48MP + 8MP + 2MP + 2MP  16MP Front Camera  16.33 cm (6.43 inch)   \n",
      "5  128 GB  48MP + 8MP + 2MP + 2MP  16MP Front Camera  16.33 cm (6.43 inch)   \n",
      "6  128 GB                       -                  -  16.33 cm (6.43 inch)   \n",
      "7  128 GB                       -                  -  16.33 cm (6.43 inch)   \n",
      "\n",
      "   Display Resolution                     Processor Processor Cores   Battery  \\\n",
      "0  2400 x 1080 Pixels  MediaTek Helio P95 (MT6779V)       Octa Core  4310 mAh   \n",
      "1  2400 x 1080 Pixels  MediaTek Helio P95 (MT6779V)       Octa Core  4310 mAh   \n",
      "2  2400 x 1080 Pixels  MediaTek Helio P95 (MT6779V)       Octa Core  4310 mAh   \n",
      "3  2400 x 1080 Pixels  MediaTek Helio P95 (MT6779V)       Octa Core  4310 mAh   \n",
      "4  2400 x 1080 Pixels       MediaTek Dimensity 800U       Octa Core         -   \n",
      "5  2400 x 1080 Pixels       MediaTek Dimensity 800U       Octa Core         -   \n",
      "6  2400 x 1080$$pixel                             -               -         -   \n",
      "7  2400 x 1080$$pixel                             -               -         -   \n",
      "\n",
      "     Price                                       Product List  \n",
      "0  ₹23,490  https://www.flipkart.com/oppo-f19-pro-crystal-...  \n",
      "1  ₹21,490  https://www.flipkart.com/oppo-f19-pro-fluid-bl...  \n",
      "2  ₹23,490  https://www.flipkart.com/oppo-f19-pro-fluid-bl...  \n",
      "3  ₹21,490  https://www.flipkart.com/oppo-f19-pro-crystal-...  \n",
      "4  ₹25,990  https://www.flipkart.com/oppo-f19-pro-5g-fluid...  \n",
      "5  ₹25,990  https://www.flipkart.com/oppo-f19-pro-5g-space...  \n",
      "6  ₹24,699  https://www.flipkart.com/oppo-f19pro-fluid-bla...  \n",
      "7  ₹24,489  https://www.flipkart.com/oppo-f19pro-space-sil...  \n"
     ]
    }
   ],
   "source": [
    "for j in url_list: # opening each href one by one and scraping data\n",
    "    driver.get(j)\n",
    "    read_more=driver.find_element_by_xpath(\"//button[@class='_2KpZ6l _1FH0tX']\").click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Scraping data of Brand\n",
    "    try:\n",
    "        Brand = driver.find_element_by_xpath(\"//*[@id= 'container']/div/div[3]/div[1]/div[2]/div[9]/div[4]/div[2]/div/div/div/div[2]/div[1]\")\n",
    "        brand.append(Brand.text)\n",
    "    except NoSuchElementException as e:\n",
    "        brand.append(\"-\")\n",
    "        \n",
    "    # Scraping data of smartphone name\n",
    "    try:\n",
    "        Smartphone = driver.find_element_by_xpath(\"//span[@class = 'B_NuCI']\")\n",
    "        smartphone.append(Smartphone.text)\n",
    "    except NoSuchElementException as e:\n",
    "        smartphone.append(\"-\")\n",
    "            \n",
    "    try: # Scraping color of phone\n",
    "        Color = driver.find_element_by_xpath(\"//table[@class='_14cfVK']/tbody/tr[4]/td[2]\")\n",
    "        colour.append(Color.text)\n",
    "    except NoSuchElementException as e:\n",
    "        colour.append(\"-\")\n",
    "            \n",
    "    try: # Scraping RAM details\n",
    "        ram = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']/div[4]/table/tbody/tr[2]/td[2]/ul/li[contains(text(),'GB')]\")\n",
    "        RAM.append(ram.text)\n",
    "    except NoSuchElementException as e:\n",
    "        RAM.append(\"-\")\n",
    "        \n",
    "        \n",
    "    try: # Scraping Storage\n",
    "        Storage = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']/div[4]/table/tbody/tr[1]/td[2]/ul/li[contains(text(),'GB')]\")\n",
    "        storage.append(Storage.text)\n",
    "    except NoSuchElementException as e:\n",
    "        storage.append(\"-\")\n",
    "            \n",
    "            \n",
    "    try:# Scraping Primary Camara Details\n",
    "        pri_cam = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']/div[5]/table/tbody/tr[2]/td[2]/ul/li[contains(text(),'MP')]\")\n",
    "        Pri_Cam.append(pri_cam.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Pri_Cam.append(\"-\")\n",
    "            \n",
    "    try:# Scraping Secondary Camara Details\n",
    "        sec_cam = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']/div[5]/table/tbody/tr[5]/td[2]/ul/li[contains(text(),'MP')]\")\n",
    "        Sec_Cam.append(sec_cam.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Sec_Cam.append(\"-\")\n",
    "            \n",
    "    try:# Scraping Display Size Details\n",
    "        display = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']/div[2]/table/tbody/tr[1]/td[2]/ul/li[contains(text(),'inch')]\")\n",
    "        Display_Size.append(display.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Display_Size.append(\"-\")\n",
    "            \n",
    "    try: # Scraping Display Resolution Details\n",
    "        dis_res = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']/div[2]/table/tbody/tr[2]/td[2]/ul/li[contains(text(),'x')]\")\n",
    "        Display_Resolution.append(dis_res.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Display_Resolution.append(\"-\")\n",
    "            \n",
    "    try: # Scraping Processor Details\n",
    "        processor = driver.find_element_by_xpath(\"//*[@id= 'container']/div/div[3]/div[1]/div[2]/div[9]/div[5]/div/div[2]/div[1]/div[3]/table/tbody/tr[2]/td[2]/ul/li\")\n",
    "        Processor.append(processor.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Processor.append(\"-\")\n",
    "            \n",
    "    try: # Scraping processor cores Details\n",
    "        processor_cores= driver.find_element_by_xpath(\"//div[@class='_1UhVsV']/div[3]/table/tbody/tr[3]/td[2]/ul/li[contains(text(),'Core')]\")\n",
    "        Processor_Cores.append(processor_cores.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Processor_Cores.append(\"-\") \n",
    "            \n",
    "    try: # Scraping Battery Details\n",
    "        battery = driver.find_element_by_xpath(\"//div[@class='_1UhVsV']/div[9]/table/tbody/tr/td[2]/ul/li[contains(text(),'mAh')]\")\n",
    "        Battery.append(battery.text)\n",
    "    except NoSuchElementException as e:\n",
    "        Battery.append(\"-\")\n",
    "            \n",
    "    try: # Scraping Price Details\n",
    "        Price =  driver.find_element_by_xpath(\"//div[@class = '_30jeq3 _16Jk6d']\")\n",
    "        price.append(Price.text)\n",
    "    except NoSuchElementException as e:\n",
    "        price.append(\"-\")\n",
    "    \n",
    "        \n",
    "            \n",
    "flipkart = pd.DataFrame({}) # Creating dataframe to save scraped data\n",
    "flipkart['Brand'] = brand[0:8]\n",
    "flipkart['Description'] = smartphone[0:8]\n",
    "flipkart['Color'] = colour[0:8]\n",
    "flipkart['RAM'] = RAM[0:8]\n",
    "flipkart['Storage'] = storage[0:8]\n",
    "flipkart['Primary Camara'] = Pri_Cam[0:8]\n",
    "flipkart['Secondary Camera'] = Sec_Cam[0:8]\n",
    "flipkart['Display Size'] = Display_Size[0:8]\n",
    "flipkart['Display Resolution'] = Display_Resolution[0:8]\n",
    "flipkart['Processor'] = Processor[0:8]\n",
    "flipkart['Processor Cores'] = Processor_Cores[0:8]\n",
    "flipkart['Battery'] = Battery[0:8]\n",
    "flipkart['Price'] = price[0:8]\n",
    "flipkart['Product List'] = url_list[0:8] \n",
    "print(flipkart)\n",
    "\n",
    "#creating csv\n",
    "df.to_csv(\"flipkart_smartphones.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5) Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter City Name : Bangalore\n",
      "Latitude =  12.9538477\n",
      "Longitude =  77.3507288\n"
     ]
    }
   ],
   "source": [
    "def google_maps(): # Defining google maps\n",
    "    driver = webdriver.Chrome(\"C://chromedriver.exe\") # Loading chrome driver\n",
    "    driver.get(\"https://www.google.com/maps/\") # opening link\n",
    "    \n",
    "    city = input(\"Please enter City Name : \") # taking user input for city name\n",
    "    driver.find_element_by_xpath(\"//input[@class = 'tactile-searchbox-input']\").send_keys(city) # sending city name to the search name\n",
    "    \n",
    "    search = driver.find_element_by_xpath(\"//button[@class = 'searchbox-searchbutton']\").click() # Starting search\n",
    "    \n",
    "    time.sleep(5)\n",
    "        \n",
    "    coordinates = driver.current_url.split(\"@\")[1].split(\",\") # retreiving and spliting the url\n",
    "    print(\"Latitude = \",coordinates[0])\n",
    "    print(\"Longitude = \",coordinates[1])\n",
    "\n",
    "# Calling function\n",
    "google_maps()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6) Write a program to scrap details of all the funding deals for second quarter (i.e. July 20 – September 20) from trak.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating open lists\n",
    "Date=[]\n",
    "Startup=[]\n",
    "Industry=[]\n",
    "SubVertical=[]\n",
    "Location=[]\n",
    "Investor=[]\n",
    "Investment=[]\n",
    "Amount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date          Startup             Industry  \\\n",
      "0    15/07/2020         Flipkart           E-commerce   \n",
      "1    16/07/2020          Vedantu              EduTech   \n",
      "2    16/07/2020             Crio              EduTech   \n",
      "3    14/07/2020          goDutch              FinTech   \n",
      "4    13/07/2020         Mystifly  Airfare Marketplace   \n",
      "..          ...              ...                  ...   \n",
      "118  07/09/2020              1mg           E-commerce   \n",
      "119  31/08/2020            mfine           HealthTech   \n",
      "120  31/08/2020             Apna      Human Resources   \n",
      "121  03/09/2020          Railofy       Transportation   \n",
      "122  08/09/2020  Cell Propulsion           Automobile   \n",
      "\n",
      "                                           SubVertical  \\\n",
      "0                                           E-commerce   \n",
      "1                                      Online Tutoring   \n",
      "2                     Learning Platform for Developers   \n",
      "3                                       Group Payments   \n",
      "4    Ticketing, Airline Retailing, and Post-Ticketi...   \n",
      "..                                                 ...   \n",
      "118                                    Online Pharmacy   \n",
      "119                      On-Demand Healthcare Services   \n",
      "120                               Recruitment Platform   \n",
      "121                       WL & RAC protection platform   \n",
      "122                        Electric Mobility Solutions   \n",
      "\n",
      "                    Location  \\\n",
      "0                  Bangalore   \n",
      "1                  Bangalore   \n",
      "2                  Bangalore   \n",
      "3                     Mumbai   \n",
      "4    Singapore and Bangalore   \n",
      "..                       ...   \n",
      "118                  Gurgaon   \n",
      "119                Bangalore   \n",
      "120                Bangalore   \n",
      "121                   Mumbai   \n",
      "122                Bangalore   \n",
      "\n",
      "                                              Investor    Investment  \\\n",
      "0                                          Walmart Inc           M&A   \n",
      "1                                    Coatue Management      Series D   \n",
      "2                                          021 Capital  pre-Series A   \n",
      "3    Matrix India,Y Combinator, Global Founders Cap...          Seed   \n",
      "4                                     Recruit Co. Ltd.  pre-Series B   \n",
      "..                                                 ...           ...   \n",
      "118         Gaja Capital, Tata Capital, Partners Group   In Progress   \n",
      "119                                   Caretech Pte Inc      Series B   \n",
      "120         Lightspeed India and Sequoia Capital India      Series A   \n",
      "121                                  Chiratae Ventures          Seed   \n",
      "122                         growX Ventures and Micelio  pre-Series A   \n",
      "\n",
      "            Amount  \n",
      "0    1,200,000,000  \n",
      "1      100,000,000  \n",
      "2          934,160  \n",
      "3        1,700,000  \n",
      "4        3,300,000  \n",
      "..             ...  \n",
      "118    100,000,000  \n",
      "119      5,400,000  \n",
      "120      8,000,000  \n",
      "121        950,000  \n",
      "122             NA  \n",
      "\n",
      "[123 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(\"C://chromedriver.exe\") # Loading chrome driver\n",
    "driver.get(\"https://trak.in/\") # url from whic we have to scrape the data\n",
    "\n",
    "# opening funding deals page\n",
    "funding_deals = driver.find_element_by_xpath(\"//li[@id = 'menu-item-51510']/a\").get_attribute('href') \n",
    "driver.get(funding_deals)\n",
    "\n",
    "\n",
    "for i in range(48,51): \n",
    "    time.sleep(5)\n",
    "    dropdown = driver.find_element_by_xpath(f\"//div[@class = 'dataTables_length']//select[@name = 'tablepress-{i}_length']\")\n",
    "    dropdown.send_keys(\"100\")\n",
    "    \n",
    "    # Extracting dates\n",
    "    dt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[2]'.format(i))\n",
    "    for d in dt:\n",
    "        Date.append(d.text)\n",
    "    \n",
    "    # Extracting Startup Name\n",
    "    sn = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[3]'.format(i))\n",
    "    for n in sn:\n",
    "        Startup.append(n.text)\n",
    "    \n",
    "    # Extracting Industry Name\n",
    "    ind = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[4]'.format(i))\n",
    "    for n in ind:\n",
    "        Industry.append(n.text)\n",
    "    \n",
    "    # Extracting Sub Vertical   \n",
    "    sv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[5]'.format(i))\n",
    "    for s in sv:\n",
    "        SubVertical.append(s.text)\n",
    "\n",
    "    # Extracting Location\n",
    "    loc = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[6]'.format(i))\n",
    "    for l in loc:\n",
    "        Location.append(l.text)\n",
    "    \n",
    "    # Extracting Investor Name\n",
    "    inv = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[7]'.format(i))\n",
    "    for n in inv:\n",
    "        Investor.append(n.text)\n",
    "    \n",
    "    # Extracting Investment\n",
    "    invt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[8]'.format(i))\n",
    "    for n in invt:\n",
    "        Investment.append(n.text)\n",
    "    \n",
    "    # Extracting Amount\n",
    "    amt = driver.find_elements_by_xpath('//table[@id=\"tablepress-{}\"]/tbody/tr/td[9]'.format(i))\n",
    "    for a in amt:\n",
    "        Amount.append(a.text)\n",
    "        \n",
    "driver.close()\n",
    "\n",
    "# Saving scraped data into dataframe\n",
    "reports = pd.DataFrame({})\n",
    "reports['Date']=Date\n",
    "reports['Startup']=Startup \n",
    "reports['Industry']=Industry\n",
    "reports['SubVertical']=SubVertical\n",
    "reports['Location']=Location\n",
    "reports['Investor']=Investor\n",
    "reports['Investment']=Investment\n",
    "reports['Amount']=Amount\n",
    "print(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7) Write a program to scrap all the available details of top 10 gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating openlist\n",
    "prod_name=[]\n",
    "OS=[]\n",
    "display_specs=[]\n",
    "processor_specs=[]\n",
    "HDD=[]\n",
    "RAM=[]\n",
    "weight=[]\n",
    "GPU=[]\n",
    "full_specs=[]\n",
    "dimension=[]\n",
    "Price=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"C://chromedriver.exe\") # Loading chrome driver\n",
    "driver.get(\"https://www.digit.in/top-products/best-gaming-laptops-40.html/\") # Url from where we will scrape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting laptop name\n",
    "name=driver.find_elements_by_xpath(\"//div[@class='right-container']/div/a/h3\")\n",
    "for i in name:\n",
    "    prod_name.append(i.text)\n",
    "\n",
    "\n",
    "# Extracting Operating System type\n",
    "OS_type=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[1]/div/div\")\n",
    "for i in OS_type:\n",
    "    OS.append(i.text)\n",
    "\n",
    "# Extracting display details\n",
    "display=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[2]/div/div\")\n",
    "for i in display:\n",
    "    display_specs.append(i.text)\n",
    "\n",
    "# Extracting processor details\n",
    "processor=driver.find_elements_by_xpath(\"//div[@class='product-detail']/div/ul/li[3]/div/div\")\n",
    "for i in processor:\n",
    "    processor_specs.append(i.text)\n",
    "\n",
    "# Extracting weight of laptops\n",
    "weights=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[1]\")\n",
    "weights_specs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[3]\")\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    if weights[i].text=='Weight':\n",
    "        weight.append(weights_specs[i].text)\n",
    "        \n",
    "# Extracting memory details\n",
    "memory=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[1]\")\n",
    "memory_specs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details'][1]/table/tbody/tr[6]/td[3]\")\n",
    "\n",
    "# seperating HDD and RAM details\n",
    "for i in range(len(memory)):\n",
    "    if memory[i].text=='Memory':\n",
    "        HDD.append(memory_specs[i].text.split('/')[0])\n",
    "        RAM.append(memory_specs[i].text.split('/')[1])\n",
    "    else:\n",
    "        HDD.append('-')\n",
    "        RAM.append('-')\n",
    "\n",
    "# Extracting dimensions od laptops\n",
    "dims=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[1]\")\n",
    "dims_specs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[3]\")\n",
    "\n",
    "for i in range(len(dims)):\n",
    "    if dims[i].text=='Dimension':\n",
    "        dimension.append(dims_specs[i].text)\n",
    "\n",
    "# Extracting Graphics Processor\n",
    "GPs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[1]\")\n",
    "GPs_specs=driver.find_elements_by_xpath(\"//div[@class='Spcs-details']/table/tbody/tr/td[3]\")\n",
    "\n",
    "for i in range(len(GPs)):\n",
    "    if GPs[i].text=='Graphics Processor':\n",
    "        GPU.append(GPs_specs[i].text)\n",
    "        \n",
    "urls=driver.find_elements_by_xpath(\"//div[@class='full-specs']/span\")\n",
    "for i in urls:\n",
    "    if i.get_attribute('data-href'):\n",
    "        full_specs.append(i.get_attribute('data-href'))\n",
    "        \n",
    "# Extracting price of Laptops\n",
    "\n",
    "for i in full_specs:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        prices=driver.find_element_by_xpath(\"//div[@class='Block-price']/b\")  #Getting price tags\n",
    "        Price.append(prices.text)  #Extracting text\n",
    "    except NoSuchElementException as e:   #Running an exception if the price is not available\n",
    "         Price.append(\"-\")    #Message to be printed in places where the price is not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Product Name                      OS  \\\n",
      "0      MSI GT76 TITAN DT 9SG          WINDOWS 10 PRO   \n",
      "1      ALIENWARE 17 AREA-51M          WINDOWS 10 PRO   \n",
      "2            HP OMEN 15 2020         WINDOWS 10 HOME   \n",
      "3          ASUS ZEPHYRUS G14         WINDOWS 10 HOME   \n",
      "4         LENOVO LEGION Y540         WINDOWS 10 HOME   \n",
      "5  ASUS ROG ZEPHYRUS G GA502         WINDOWS 10 HOME   \n",
      "6  ASUS ROG ZEPHYRUS S GX531         WINDOWS 10 HOME   \n",
      "7   MSI GT83VR 7RE TITAN SLI  WINDOWS 10 HOME 64 BIT   \n",
      "8   ASUS ROG ZEPHYRUS DUO 15              WINDOWS 10   \n",
      "9                    DELL G3    WINDOWS 10 HOME PLUS   \n",
      "\n",
      "                     Display  \\\n",
      "0      17.3\" (UHD 3840X2160)   \n",
      "1  17.3\" (FHD (1920 X 1080))   \n",
      "2        15.6\" (1920 X 1080)   \n",
      "3          14\" (1920 X 1080)   \n",
      "4        15.6\" (1920 X 1080)   \n",
      "5        15.6\" (1920 X 1080)   \n",
      "6        15.6\" (1920 X 1080)   \n",
      "7        18.4\" (1920 X 1080)   \n",
      "8        15.6\" (3840 X 1100)   \n",
      "9               15.6 MP | NA   \n",
      "\n",
      "                                           Processor            HDD  \\\n",
      "0             INTEL 9TH GEN CORE I9-9900K | 5000 MHZ       1 TB HDD   \n",
      "1             INTEL 9TH GEN CORE I9-9900K | 5000 MHZ  1 TB PCIe SSD   \n",
      "2                  INTEL I7-10750H 10TH GEN | 1.6GHZ     512 GB SSD   \n",
      "3               AMD 3RD GENERATION RYZEN 9 | 3.3 GHZ       1 TB SSD   \n",
      "4       9TH GENERATION CORE INTEL I7-9750H | 2.6 GHZ       1 TB SSD   \n",
      "5              AMD RYZEN 7 QUAD CORE 3750H | 2.3 GHZ     512 GB SSD   \n",
      "6              9TH GEN INTEL CORE I7-8750H | 2.2 GHZ      512GB SSD   \n",
      "7  INTEL CM238 CORE I7-7820HK+CM238 7TH GEN | 3.5GHZ    1.5 TB SATA   \n",
      "8                 INTEL CORE I7 10TH GEN 10875H | NA     512 GB SSD   \n",
      "9               8TH GEN INTEL CORE I5-8300H | 2.3GHZ        1TB HDD   \n",
      "\n",
      "            RAM   Weight                               Dimension  \\\n",
      "0  64 GBGB DDR4   4.2 kg                    397 x 330 x 33~42 mm   \n",
      "1     32GB DDR4  3.87 Kg            42 mm x 402.6 mm x 319.14 mm   \n",
      "2  16 GBGB DDR4     5.40                     14.09 x 9.44 x 0.89   \n",
      "3  16 GBGB DDR4     1.65                       32.5 x 22.1 x 1.8   \n",
      "4      8GB DDR4      2.3                  365mm x 260mm x 25.9mm   \n",
      "5     16GB DDR4      2.2                        360 x 252 x 20.4   \n",
      "6  16 GBGB DDR4      2.1  360 (W) x 268 (D) x 15.35~16.15 (H) mm   \n",
      "7     64GB DDR4      5.5                          458 x 339 x 69   \n",
      "8   4 GBGB DDR4      2.4                 268.30 x 360.00 x 20.90   \n",
      "9      8GB DDR4     2.53                        22.7 X 380 X 258   \n",
      "\n",
      "                   Graphic Processor    Price  \n",
      "0            NVIDIA GeForce RTX 2080  379,990  \n",
      "1            NVIDIA GeForce RTX 2080  422,000  \n",
      "2          Nvidia GeForce GTX 1650Ti  117,790  \n",
      "3            NVIDIA GeForce RTX 2060  164,990  \n",
      "4          NVIDIA® GeForce RTX™ 2060   79,990  \n",
      "5         NVIDIA Geforce GTX 1660 Ti   79,990  \n",
      "6  NVIDIA® GeForce RTX™ 2070 (Max-Q)  239,990  \n",
      "7                       Dual GTX1070  349,990  \n",
      "8      NVIDIA GeForce RTX 2070 Max-Q  244,990  \n",
      "9            NVidia GeForce GTX 1050   73,900  \n"
     ]
    }
   ],
   "source": [
    "laptops=pd.DataFrame({})\n",
    "laptops['Product Name']= prod_name\n",
    "laptops['OS'] = OS\n",
    "laptops['Display'] = display_specs\n",
    "laptops['Processor'] = processor_specs\n",
    "laptops['HDD'] = HDD\n",
    "laptops['RAM'] = RAM\n",
    "laptops['Weight'] = weight\n",
    "laptops['Dimension'] = dimension\n",
    "laptops['Graphic Processor'] = GPU\n",
    "laptops['Price'] = Price\n",
    "print(laptops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
